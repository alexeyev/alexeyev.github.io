---
layout: default
---
<header class="masthead">
    <h1 class="masthead-title">
        <a href="{{ site.baseurl }}/nlp-itmo-spring-2018">NLP, NRU ITMO, Spring 2018</a>
    </h1>
    <nav class="masthead-nav">
        {% for nav in site.nav %}
        <a href="{{ nav.href }}">{{ nav.name }}</a>
        {% endfor %}
    </nav>
</header>
<div class="content list">
    <h4>Slides</h4>
    <ol>
        <li><a href="/pdf/2018-nlp-itmo/ITMO_NLP_01_INLP_IIR.pdf">Introduction + NLP for information retrival</a>
        <li><a href="/pdf/2018-nlp-itmo/ITMO_NLP_02_STRINGS_DISTANCES.pdf">String metrics, text representations, regular expressions</a>
	<li><a href="/pdf/2018-nlp-itmo/ITMO_NLP_03_LANGUAGE_MODELS_I.pdf">Language models</a>
	<li><a href="/pdf/2018-nlp-itmo/ITMO_NLP_03_LANGUAGE_MODELS_II.pdf">Language models, Markov chains</a>
	<li><a href="/pdf/2018-nlp-itmo/ITMO_NLP_05_SHANNON_&_VECTOR_SEMANTICS.pdf">Markov chains, Information Theory elements</a>
	<li><a href="/pdf/2018-nlp-itmo/ITMO_NLP_07_CLUSTERING.pdf">Text clustering</a>
	<li><a href="/pdf/2018-nlp-itmo/ITMO_NLP_07_VECTOR_SEMANTICS.pdf">Vector semantics-1</a>
	<li><a href="/pdf/2018-nlp-itmo/ITMO_NLP_07_VECTOR_SEMANTICS-again.pdf">Vector semantics-2</a>
	<li><a href="/pdf/2018-nlp-itmo/ITMO_NLP_08_DUPLICATE_DETECTION.pdf">Duplicate text detenction: MinHash, LSH</a>
	<li><a href="/pdf/2018-nlp-itmo/ITMO_NLP_08_TOPIC_MODELING.pdf">Topic modeling</a>
    </ol>
</div> 

<div class="content list">
	<h4>Written exam questions</h4>
	<ol>
1. Zipf's Law, its importance for NLP. Language processing in information retrieval: lemmatization, stemming, Boolean search, inverted indices, execution of Boolean queries on them, skip-lists.
2. Language processing in information retrieval: vector space model, cosine distance, TF-IDF. Common ways of representing texts for machine learning tasks.
3. String distances and the algorithms for their computation: the Hamming distance, the Jaro-Winkler distance, the Levenshtein distance, the smallest common subsequence, the Jaccard distance for character N-grams. Indices for typos detection/correction in words.
4. Edit distances (definitions only). Regular expressions: basic constructions, recommendations for use.
5. Markov chains. Ergodic theorem. PageRank and Markov chains. Direct applications in the text analysis.
6. Elements of information theory: self-information, bit, pointwise mutual information, Kullback-Leibler divergence, Shannon entropy, its interpretations. Cross-entropy. Example of an application: collocations extraction.
7. Language modeling. N-gram models. Perplexity. The reasons for doing smoothing. Additive (Laplace) smoothing. Interpolation and backoff. The ideas on which the Kneser-Ney smoothing is based.
8. Vector semantics: term-document matrices, term-context matrices, HAL. SVD, LSA, NMF. Methods for quality evaluation of vector semantics models.
9. Vector semantics: what is word2vec (the core principles of the SGNS algorithm and its relationship with matrix factorization), 
		word2vec as a neural network. Methods for quality evaluation of vector semantics models.
10. Clustering: types of clustering algorithms. KMeans, agglomerative and divisive clustering 
		(+ ways of estimating the distances between clusters), DBSCAN. 
		Limitations and areas of applicability of all algorithms. 
		Methods clustering quality evaluation, the shortcomings of each.
11. Duplicates search: statement of the problem, description of the MinHash algorithm. 
		"Permutations generation" in practice.
12. Topic modeling. LSA, pLSA, LDA, ARTM. Advantages and disadvantages of each method. 
		Topic modeling quality evaluation (perplexity, coherence and methods with experts involved).
13. Classification. Binary classification quality evaluation. 
		Metric classification methods. Logical methods of classification. Linear classification methods.
14. Quality evaluation of machine learning models (why divide the data set into three parts). 
		Classification. Multi-class classification quality evaluation. 
		Naive Bayes Classifier. Ensembles of models of machine learning.
15. Sequence tagging. PoS tagging. Named entity recognition. Hidden Markov models. 
		Estimation of the probability of a sequence of states. 
		Estimation of the probability of a sequence of observations. 
		Quality evaluation.
16. Sequence tagging. PoS tagging. Named entity recognition. Hidden Markov models. 
		Decoding of the most probable sequence of states. Quality evaluation.
17. Syntax parsing. Syntax description approaches. 
		Phrase structure grammar: the principles. Formal grammar. 
		Chomsky Normal Form. Cocke-Kasami-Younger algorithm, its complexity. Parsing quality evaluation.
18. Syntax parsing. Syntax description approaches. Phrase structure grammar: the principles. 
		Probabilistic context-free grammar. Cocke-Kasami-Younger algorithm for PCFG, its complexity. 
		Parsing quality evaluation.
19. Syntax parsing. Syntax description approaches. Dependency grammar, core principles. 
		Parsing quality evaluation. Transition-based dependency parsing: how it works. 
		The algorithm (everything but the 'oracle').
20. Machine translation. Evaluation of the quality of machine translation. 
		BLEU. IBM Models 1,2 - core principles. </ol>
	    </div>
