---
layout: default
---
<header class="masthead">
    <h1 class="masthead-title">
        <a href="{{ site.baseurl }}">NLP, NRU ITMO, Spring 2019</a>
    </h1>
    <nav class="masthead-nav">
        {% for nav in site.nav %}
        <a href="{{ nav.href }}">{{ nav.name }}</a>
        {% endfor %}
    </nav>
</header>
<div class="content list">
    <h4>Slides</h4>
    <ol>
        <li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_01_INLP_IIR.pdf">Introduction + NLP for information retrival</a>
        <li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_02_TEXT_CLASSIFICATION.pdf">Text representations, model evaluation and text classification</a>
        <li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_03_STRINGS_DISTANCES.pdf">String distances, regular expressions</a>
	<li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_04_LANGUAGE_MODELING.pdf">Language modeling</a>
	<li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_05_MARKOV_CHAINS_AND_INFORMATION_THEORY.pdf">Markov chains, information theory</a>
	<li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_06_VECTOR_SEMANTICS.pdf">Vector semantics</a>
	<li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_07_CLUSTERING.pdf">Clustering</a>
	<li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_08_TOPIC_MODELING.pdf">Topic modeling</a>
	<li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_09_DUPLICATE_DETECTION.pdf">Duplicate objects detection</a>
	<li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_10_SEQLEARN.pdf">Sequence tagging</a>
	<li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_11_SYNTAX_PSG.pdf">Syntax: PSG and others</a>
	<li><a href="/pdf/2019-nlp-itmo/ITMO_NLP_12_2017-SYNTAX-II.pdf">Syntax: dependency grammar</a>
	<li><a href="http://val.maly.hk/">Valentin Malykh</a>:&nbsp;&nbsp;<a href="/pdf/2019-nlp-itmo/ITMO_NLP_13_CNN4NLP.pdf">guest lecture: Convolutional NNs for NLP</a>
	<li><a href="http://val.maly.hk/">Valentin Malykh</a>:&nbsp;&nbsp;<a href="/pdf/2019-nlp-itmo/ITMO_NLP_14_ELMO_AND_ATTENTION.pdf">guest lecture: Attention mechanism (+more) for NLP</a>	
	
    </ol>
</div> 

<div class="content list">
	<h4>Written exam questions</h4>
	<ol>
<li> Zipf's Law, its importance for NLP. Language processing in information retrieval: lemmatization, 
	stemming, Boolean search, inverted indices, execution of Boolean queries on them, skip-lists.
<li> Language processing in information retrieval: vector space model, cosine distance, TF-IDF.
	Common ways of representing texts for machine learning tasks.
<li>  String distances and the algorithms for their computation: the Hamming distance, <strike>the Jaro-Winkler distance</strike>, 
	the Levenshtein distance, the longest common subsequence, the Jaccard distance for character N-grams. 
	Indices for typos detection/correction in words.
<li>  Edit distances (definitions only). Regular expressions: basic constructions, recommendations for use.
<li>  Markov chains. Ergodic theorem. PageRank and Markov chains. Direct applications in the text analysis.
<li>  Elements of information theory: self-information, bit, pointwise mutual information, Kullback-Leibler divergence,
	Shannon entropy, its interpretations. Cross-entropy. Example of an application: collocations extraction.
<li>  Language modeling. N-gram models. Perplexity. The reasons for doing smoothing. Additive (Laplace) smoothing. 
	Interpolation and backoff. The ideas on which the Kneser-Ney smoothing is based.
<li> Vector semantics: term-document matrices, term-context matrices, HAL. SVD, LSA, NMF. Methods for quality evaluation 
	of vector semantics models.
<li>  Vector semantics: what is word2vec (the core principles of the SGNS algorithm and its relationship with 
	matrix factorization), 
		word2vec as a neural network. Methods for quality evaluation of vector semantics models.
<li>  Clustering: types of clustering algorithms. KMeans, agglomerative and divisive clustering 
		(+ ways of estimating the distances between clusters), DBSCAN. 
		Limitations and areas of applicability of all algorithms. 
		Methods clustering quality evaluation, the shortcomings of each.
<li>  Duplicates search: statement of the problem, description of the MinHash algorithm. Probability of hashes matching 
	is equal to Jaccard similarity (with proof). <!--"Permutations generation" in practice.-->
<li>  Topic modeling. LSA, pLSA, LDA, ARTM. Advantages and disadvantages of each method. 
		Topic modeling quality evaluation (perplexity, coherence and methods with experts involved).
<li>  Classification. Binary classification quality evaluation. 
		Metric classification methods. Logical methods of classification. Linear classification methods.
<li>  Quality evaluation of machine learning models (why divide the data set into three parts). 
		Classification. Multi-class classification quality evaluation. 
		Naive Bayes Classifier. Ensembles of models of machine learning.
<li>  Sequence tagging. PoS tagging. Named entity recognition. Hidden Markov models. 
		Estimation of the probability of a sequence of states. 
		Estimation of the probability of a sequence of observations. 
		Quality evaluation.
<li>  Sequence tagging. PoS tagging. Named entity recognition. Hidden Markov models. 
		Decoding of the most probable sequence of states (Veterbi algorithm without proof). Quality evaluation.
<li>  Sequence tagging. PoS tagging. Named entity recognition. Structured perceptron. 
		Structured perceptron training. Sequente tagging quality evaluation.
<li>  Syntax parsing. Syntax description approaches. 
		Phrase structure grammar: the principles. Formal grammar. 
		Chomsky Normal Form. Cocke-Kasami-Younger algorithm, its complexity. Parsing quality evaluation.
<li>  Syntax parsing. Syntax description approaches. Phrase structure grammar: the principles. 
		Probabilistic context-free grammar. Cocke-Kasami-Younger algorithm for PCFG (without proof), its complexity. 
		Parsing quality evaluation.
<li>  Syntax parsing. Syntax description approaches. Dependency grammar, core principles. 
		Parsing quality evaluation. Transition-based dependency parsing: how it works. 
		The algorithm (everything but the 'oracle').
<!--<li>  ... 
	
 <li>  Machine translation. Evaluation of the quality of machine translation. 
		BLEU. IBM Models 1,2 - core principles. 
 -->
	</ol>
</div>
